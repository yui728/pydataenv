{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 スクレイピング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.1 スクレイピングとは\n",
    "* インターネット上のWebページから情報を取得すること\n",
    "* Webページの内容をプログラムで使用するために、必要とする要素のみを抜き出すこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2 スクレイピングの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Requests](http://docs.python-requests.org)\n",
    "* Webブラウザの代わりにWebサイトにアクセスし、HTTPでデータの送受信を行う"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* HTMLやXMLを解析しデータを取り出すためのライブラリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 Webページをダウンロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CodeZine](https://codezine.jp/)の情報を取得する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requestsを用いてCodeZineにアクセスする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get('https://codezine.jp/') # URLにアクセスする\n",
    "print(type(r))\n",
    "print(r.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ページの内容（HTML）を取得し、`<titie>`タグと`<h1>`タグの要素を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>CodeZine（コードジン）</title>\n",
      "<h1><a href=\"/\"><img src=\"/lib/img/cmn/cmn-header-logo.png\" alt=\"CodeZine（コードジン）\" ></a></h1>\n"
     ]
    }
   ],
   "source": [
    "text = r.text # HTMLのソースコードを取得する\n",
    "for line in text.split('\\n'):\n",
    "    if '<title>' in line or '<h1>' in line:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.4 Webページから要素を抜き出す\n",
    "* BeautifulSoup4を使うと、HTMLを構文解析して任意の要素を取り出しことができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CodeZineから取得したHTMLを構文解析し、指定の要素を取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>CodeZine（コードジン）</title>\n",
      "<h1><a href=\"/\"><img alt=\"CodeZine（コードジン）\" src=\"/lib/img/cmn/cmn-header-logo.png\"/></a></h1>\n",
      "CodeZine（コードジン）\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTMLを解析したオブジェクトを生成\n",
    "soup = BeautifulSoup(text, 'html.parser')\n",
    "print(soup.title) # <title>タグの情報を取得\n",
    "print(soup.h1) # <h1>タグの情報を取得\n",
    "# h1タグの中のaタグの中のimgタグのalt属性\n",
    "print(soup.h1.a.img['alt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bs4.BeautifulSoup.find_allメソッド\n",
    "* 引数で指定したタグの要素をすべて取得する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すべてのaタグを取得し、先頭5件分のタイトルとリンク先を出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aタグの数： 169\n",
      "タイトル： このページの本文へ移動\n",
      "リンク： #contents\n",
      "タイトル： 企業IT\n",
      "リンク： https://enterprisezine.jp/\n",
      "タイトル： 開発\n",
      "リンク： https://codezine.jp/\n",
      "タイトル： データベース\n",
      "リンク： https://enterprisezine.jp/dbonline/\n",
      "タイトル： セキュリティ\n",
      "リンク： https://enterprisezine.jp/securityonline/\n"
     ]
    }
   ],
   "source": [
    "atags = soup.find_all('a') # すべてのaタグを取得\n",
    "print('aタグの数：', len(atags)) # aタグの数を取得\n",
    "for atag in atags[:5]:\n",
    "    print('タイトル：', atag.text) # aタグのテキストを取得\n",
    "    print('リンク：', atag['href']) # aタグのリンクを取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.5 記事の一覧を抜き出す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CodeZineの記事一覧の構造\n",
    "* 記事自体が`<ul class=\"catList\">`の中にある\n",
    "* 1つの記事はその中の`<li>`単位\n",
    "* `<div class=\"day\">`の中に日付\n",
    "* `<h2>`の中の`<a>`の中にタイトルとリンク先\n",
    "* `<ul class=\"tag\">`の中にタグ情報があり、各タグごとに`<li>`がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CodeZineの記事一覧を抜き出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "r = requests.get('https://codezine.jp/article/tag/223') # Pythonの記事一覧を取得\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "articles = [] # 各記事の情報を格納するリスト\n",
    "\n",
    "# CSSセレクターで<ul class=\"catList\"><li>を取得\n",
    "lis = soup.select('ul.catList > li')\n",
    "for li in lis:\n",
    "    # 日付の文字列を取得\n",
    "    day = li.find('div', class_ = 'day').text.strip()\n",
    "    # 日付をdatetimeに変換\n",
    "    published = datetime.strptime(day, '%Y/%m/%d')\n",
    "    h2_tag = li.find('h2') # h2タグを取得\n",
    "    title = h2_tag.text # タイトルを取得\n",
    "    url = h2_tag.a['href'] # URLを取得\n",
    "    \n",
    "    tag_list = li.select('ul.tag > li') # タグのli要素を取得\n",
    "    # タグのリストを生成\n",
    "    tags = [tag.text.strip() for tag in tag_list]\n",
    "    \n",
    "    article = {\n",
    "        'published': published,\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'tags': tags\n",
    "    }\n",
    "    articles.append(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "抜き出した記事一覧の先頭3件を出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'published': datetime.datetime(2019, 1, 30, 0, 0),\n",
       "  'title': '『Pythonで動かして学ぶ 自然言語処理入門』から自然言語処理の概要を紹介',\n",
       "  'url': '/article/detail/11335',\n",
       "  'tags': ['Python', '人工知能']},\n",
       " {'published': datetime.datetime(2019, 1, 23, 0, 0),\n",
       "  'title': 'テキストマイニングや評判分析も 『Pythonで動かして学ぶ 自然言語処理入門』',\n",
       "  'url': '/article/detail/11293',\n",
       "  'tags': ['Python', '機械学習']},\n",
       " {'published': datetime.datetime(2019, 1, 11, 0, 0),\n",
       "  'title': 'なぜ機械学習にPythonが強いのか、Pythonで並行処理をするコツを伝授【PyData.tokyo】',\n",
       "  'url': '/article/detail/11197',\n",
       "  'tags': ['Python', 'データ処理']}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成した記事一覧の辞書を、DataFrameに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(articles) # 辞書をDataFrameに変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変換したDataFrameを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "      <th>tags</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>[Python, 人工知能]</td>\n",
       "      <td>『Pythonで動かして学ぶ 自然言語処理入門』から自然言語処理の概要を紹介</td>\n",
       "      <td>/article/detail/11335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-23</td>\n",
       "      <td>[Python, 機械学習]</td>\n",
       "      <td>テキストマイニングや評判分析も 『Pythonで動かして学ぶ 自然言語処理入門』</td>\n",
       "      <td>/article/detail/11293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-11</td>\n",
       "      <td>[Python, データ処理]</td>\n",
       "      <td>なぜ機械学習にPythonが強いのか、Pythonで並行処理をするコツを伝授【PyData....</td>\n",
       "      <td>/article/detail/11197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   published             tags  \\\n",
       "0 2019-01-30   [Python, 人工知能]   \n",
       "1 2019-01-23   [Python, 機械学習]   \n",
       "2 2019-01-11  [Python, データ処理]   \n",
       "\n",
       "                                               title                    url  \n",
       "0             『Pythonで動かして学ぶ 自然言語処理入門』から自然言語処理の概要を紹介  /article/detail/11335  \n",
       "1           テキストマイニングや評判分析も 『Pythonで動かして学ぶ 自然言語処理入門』  /article/detail/11293  \n",
       "2  なぜ機械学習にPythonが強いのか、Pythonで並行処理をするコツを伝授【PyData....  /article/detail/11197  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.6 スクレイピングで気をつけること\n",
    "1. Webサイトがプログラムでのアクセスを許可しているかどうか確認する\n",
    "  * [robots.txt](https://developers.google.com/search/reference/robots_txt)で確認できる\n",
    "1. 同じWebサイトに連続してアクセスしない\n",
    "  * Webサーバの負荷が高まるため"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.7 次のステップ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JavaScript対応\n",
    "* RequestsとBeautifulSoup4の組み合わせでは、JavaScriptで表示されているコンテンツの取得は出来ない\n",
    "  * 別途、[Selenium](https://www.seleniumhq.org/)とヘッドレスブラウザが必要となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スクレイピングフレームワーク：[Scrapy](https://scrapy.org/)\n",
    "* Webスクレイピングフレームワーク\n",
    "* 以下の機能を提供する\n",
    "  * クローリング機能：複数ページにアクセスする\n",
    "  * スクレイピング機能：Webページから情報を抜き出す\n",
    "* robots.txtやWebサイトへのアクセス間隔の設定に対応する"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
